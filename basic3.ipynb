{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：[【連載】LangChainの公式チュートリアルを1個ずつ地味に、地道にコツコツと【Basic編#3】](https://zenn.dev/chips0711/articles/25c11940a999a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChainのドキュメントローダー([公式参考](https://python.langchain.com/docs/how_to/#document-loaders))\n",
    "様々なデータ形式からテキストデータを読み込み、LangChainで処理できるDocumentオブジェクトに変換する機能  \n",
    "Documentオブジェクトは、テキストデータとそのメタデータを格納するための標準的な形式  \n",
    "以下のようなファイルからの読み込みに対応している他、自作のドキュメントローダーを作成することも可能  \n",
    " - CSVローダー\n",
    " - ディレクトリローダー\n",
    " - HTMLローダー\n",
    " - JSONローダー\n",
    " - Markdownローダー\n",
    " - Markdownを要素ごとに分割して読み込む\n",
    " - Officeファイルローダー\n",
    " - PDFローダー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextSplitter([公式参考](https://python.langchain.com/docs/how_to/#text-splitters))\n",
    " - テキストを意味のある単位に分割するためのインターフェース\n",
    " - コンテキストウィンドウのサイズ制限に対応したり、各チャンクを個別に処理したりすることができる  \n",
    " \n",
    "※参考サイトのコードはライブラリバージョンの関係でそのまま動作しなかったため、  \n",
    "　公式のサンプルを元に修正している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RecursiveCharacterTextSplitterの例\n",
    " - chunk_overlapパラメータで、チャンク間の重複する文字数を指定できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "チャンク 1: これは、RecursiveCharact\n",
      "チャンク 2: aracterTextSplitterの\n",
      "チャンク 3: tterの使用例です。このテキストは、チ\n",
      "チャンク 4: ストは、チャンクサイズに応じて分割されま\n",
      "チャンク 5: 分割されます。分割されたチャンクは、それ\n",
      "チャンク 6: クは、それぞれ独立して処理されます。\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# テキスト分割器を初期化\n",
    "long_text = \"これは、RecursiveCharacterTextSplitterの使用例です。このテキストは、チャンクサイズに応じて分割されます。分割されたチャンクは、それぞれ独立して処理されます。\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=20, chunk_overlap=5)\n",
    "\n",
    "# テキストを分割\n",
    "texts = text_splitter.create_documents([long_text])\n",
    "\n",
    "# 分割されたテキストを表示\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"チャンク {i+1}: {text.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MarkdownHeaderTextSplitterの例\n",
    " - headers_to_split_onパラメータで見出しレベルを指定することができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ドキュメント 1:\n",
      "これは、見出し1のセクションです。\n",
      "ドキュメント 2:\n",
      "これは、見出し1.1のセクションです。\n",
      "ドキュメント 3:\n",
      "これは、見出し1.2のセクションです。\n",
      "ドキュメント 4:\n",
      "これは、見出し2のセクションです。\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"# 見出し1\n",
    "これは、見出し1のセクションです。\n",
    "\n",
    "## 見出し1.1\n",
    "これは、見出し1.1のセクションです。\n",
    "\n",
    "## 見出し1.2\n",
    "これは、見出し1.2のセクションです。\n",
    "\n",
    "# 見出し2\n",
    "これは、見出し2のセクションです。\n",
    "\"\"\"\n",
    "\n",
    "# MarkdownHeaderTextSplitterを初期化\n",
    "# headers_to_split_on = [\"# \", \"## \"]\n",
    "headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\")]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# テキストを分割\n",
    "markdown_docs = markdown_splitter.split_text(markdown_text)\n",
    "\n",
    "# 分割されたドキュメントを表示\n",
    "for i, doc in enumerate(markdown_docs):\n",
    "    print(f\"ドキュメント {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HTMLHeaderTextSplitterの例\n",
    " - headers_to_split_onパラメータで見出しレベルを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ドキュメント 1:\n",
      "これは、見出し1のセクションです。\n",
      "ドキュメント 2:\n",
      "これは、見出し1.1のセクションです。\n",
      "ドキュメント 3:\n",
      "これは、見出し1.1.1のセクションです。\n",
      "ドキュメント 4:\n",
      "これは、見出し2のセクションです。\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_text = \"\"\"<h1>見出し1</h1>\n",
    "<p>これは、見出し1のセクションです。</p>\n",
    "\n",
    "<h2>見出し1.1</h2>\n",
    "<p>これは、見出し1.1のセクションです。</p>\n",
    "\n",
    "<h3>見出し1.1.1</h3>\n",
    "<p>これは、見出し1.1.1のセクションです。</p>\n",
    "\n",
    "<h1>見出し2</h1>\n",
    "<p>これは、見出し2のセクションです。</p>\n",
    "\"\"\"\n",
    "\n",
    "# HTMLHeaderTextSplitterを初期化\n",
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\")]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# テキストを分割\n",
    "html_docs = html_splitter.split_text(html_text)\n",
    "\n",
    "# 分割されたドキュメントを表示\n",
    "for i, doc in enumerate(html_docs):\n",
    "    print(f\"ドキュメント {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CodeSplitterの例\n",
    " - プログラミング言語に合わせて適切な分割を行える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ドキュメント 1:\n",
      "def my_function(a, b):\n",
      "    # これは関数です。\n",
      "ドキュメント 2:\n",
      "return a + b\n",
      "ドキュメント 3:\n",
      "class MyClass:\n",
      "    # これはクラスです。\n",
      "ドキュメント 4:\n",
      "def __init__(self, x):\n",
      "        self.x = x\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "python_code = \"\"\"\n",
    "def my_function(a, b):\n",
    "    # これは関数です。\n",
    "    return a + b\n",
    "\n",
    "class MyClass:\n",
    "    # これはクラスです。\n",
    "\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\"\"\"\n",
    "\n",
    "# CodeSplitterを初期化\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    "    )\n",
    "\n",
    "# コードを分割\n",
    "code_docs = code_splitter.create_documents([python_code])\n",
    "\n",
    "# 分割されたドキュメントを表示\n",
    "for i, doc in enumerate(code_docs):\n",
    "    print(f\"ドキュメント {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SemanticTextSplitterの例\n",
    " - 意味的に関連する文章をまとめてチャンク化することで、より自然な分割を行える  \n",
    "　→　分割できなかった。長い文章にするとAPI_KEYの認証エラーが出る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "チャンク 1: これは、SemanticTextSplitterの使用例です。このテキストは、意味的なまとまりに応じて分割されます。意味的な分割により、各チャンクがより自然な単位になります。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "# SemanticTextSplitterを初期化\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings(api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]))\n",
    "\n",
    "# テキストを分割\n",
    "text = \"これは、SemanticTextSplitterの使用例です。このテキストは、意味的なまとまりに応じて分割されます。意味的な分割により、各チャンクがより自然な単位になります。\"\n",
    "texts = text_splitter.create_documents([text])\n",
    "\n",
    "# 分割されたテキストを表示\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"チャンク {i+1}: {text.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding(テキストの埋め込み)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 埋め込みを活用した検索システムの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ベクトルストア\n",
    "\n",
    "埋め込みベクトルを格納・検索するためのデータベース  \n",
    "LangChainでは、Chroma、FAISS、Pineconeなど、様々なベクトルストアをサポートしている  \n",
    "以下のような共通機能がある\n",
    "\n",
    " - 埋め込みベクトルの追加:\n",
    "   - add_textsメソッドやadd_documentsメソッドを使って、埋め込みベクトルを追加できる\n",
    " - 類似度検索:\n",
    "   - similarity_searchメソッドやsimilarity_search_with_scoreメソッドを使って、クエリに類似したドキュメントを検索できる\n",
    " - その他の検索方法:\n",
    "   - ベクトルストアによっては、最大マージン関連検索（MMR）やスコア閾値検索などの検索方法も提供している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FAISS (Facebook AI Similarity Search)[[公式参考](https://python.langchain.com/docs/how_to/vectorstores/)]\n",
    " - 高速な検索性能に特化したベクトルストア\n",
    " - 大規模なデータセットに対しても高速な検索を実現できる\n",
    " - C++で実装されており、Pythonからも利用可能  \n",
    "　→　こちらも動作せず(API_KEY関連のエラー)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# AzureOpenAIEmbeddingsインスタンスを作成します\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=os.environ[\"AZURE_OPENAI_MODEL_NAME\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "# 埋め込み対象のドキュメントを作成\n",
    "documents = [\n",
    "    Document(page_content=\"犬は忠実な友達です。\", metadata={\"source\": \"ペット百科事典\"}),\n",
    "    Document(page_content=\"猫は独立心が強いです。\", metadata={\"source\": \"猫の飼い方ガイド\"}),\n",
    "]\n",
    "\n",
    "# FAISSベクトルストアを初期化\n",
    "vectorstore = FAISS.from_documents(documents, OpenAIEmbeddings(api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]))\n",
    "\n",
    "# 検索クエリを埋め込み\n",
    "query_embedding = embeddings.embed_query(\"猫\")\n",
    "\n",
    "# MMRを使って類似するドキュメントを検索\n",
    "results = vectorstore.max_marginal_relevance_search(\n",
    "    query_embedding, k=2, fetch_k=4\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"コンテンツ: {result.page_content}, メタデータ: {result.metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
